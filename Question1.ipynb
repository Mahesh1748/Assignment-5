{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Apply PCA on CC General data set\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialize PCA model with 2 components\npca = PCA(n_components=2)\n\n# Fit and transform the data using PCA\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Print the explained variance ratio\nprint('Explained variance ratio:', pca.explained_variance_ratio_)\n\n# Create a new dataframe with the transformed data\ncc_pca_df = pd.DataFrame(data=cc_pca, columns=['PC1', 'PC2'])\n\n# Print the transformed data\nprint('Transformed data:', cc_pca_df.head())",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": "Explained variance ratio: [0.28845814 0.21570572]\nTransformed data:         PC1       PC2\n0 -1.718894 -1.072938\n1 -1.169305  2.509318\n2  0.938413 -0.382586\n3 -0.907501  0.045869\n4 -1.637832 -0.684977\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Calculate silhouette score without applying pca\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the scaled data\nkmeans.fit(cc_scaled)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_scaled, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stderr",
          "text": "/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Silhouette score: 0.22588997653013274\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Calculate silhouette score applying pca\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialize PCA model with 2 components\npca = PCA(n_components=2)\n\n# Fit and transform the data using PCA\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the PCA transformed data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.46720661083892595\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Perform Scaling+PCA+K-Means and report performance with 2 clusters in kmeans\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Apply PCA\npca = PCA(n_components=2)\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the PCA data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.46357928828763445\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Perform Scaling+PCA+K-Means and report performance with 3 clusters in kmeans\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Apply PCA\npca = PCA(n_components=2)\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\n\n# Fit the k-means model on the PCA data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.4533204013901623\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}